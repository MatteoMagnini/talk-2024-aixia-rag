@string{ amstrans = "American Mathematical Society Translations" }
@string{ amstrans = "Amer. Math. Soc. Transl." }
@string{ bullams = "Bulletin of the American Mathematical Society" }
@string{ bullams = "Bull. Amer. Math. Soc." }
@string{ procams = "Proceedings of the American Mathematical Society" }
@string{ procams = "Proc. Amer. Math. Soc." }
@string{ transams = "Transactions of the American Mathematical Society" }
@string{ transams = "Trans. Amer. Math. Soc." }
@string{ cacm = "Communications of the {ACM}" }
@string{ cacm = "Commun. {ACM}" }
@string{ compserv = "Comput. Surveys" }
@string{ jacm = "J. ACM" }
@string{ acmmathsoft = "{ACM} Transactions on Mathematical Software" }
@string{ acmmathsoft = "{ACM} Trans. Math. Software" }
@string{ signum = "{ACM} {SIGNUM} Newsletter" }
@string{ signum = "{ACM} {SIGNUM} Newslett." }
@string{ amersocio = "American Journal of Sociology" }
@string{ amerstatassoc = "Journal of the American Statistical Association" }
@string{ amerstatassoc = "J. Amer. Statist. Assoc." }
@string{ applmathcomp = "Applied Mathematics and Computation" }
@string{ applmathcomp = "Appl. Math. Comput." }
@string{ amermathmonthly = "American Mathematical Monthly" }
@string{ amermathmonthly = "Amer. Math. Monthly" }
@string{ bit = "{BIT}" }
@string{ britstatpsych = "British Journal of Mathematical and Statistical Psychology" }
@string{ britstatpsych = "Brit. J. Math. Statist. Psych." }
@string{ canmathbull = "Canadian Mathematical Bulletin" }
@string{ canmathbull = "Canad. Math. Bull." }
@string{ compapplmath = "Journal of Computational and Applied Mathematics" }
@string{ compapplmath = "J. Comput. Appl. Math." }
@string{ compphys = "Journal of Computational Physics" }
@string{ compphys = "J. Comput. Phys." }
@string{ compstruct = "Computers and Structures" }
@string{ compstruct = "Comput. \& Structures" }
@string{ compjour = "The Computer Journal" }
@string{ compjour = "Comput. J." }
@string{ compsyssci = "Journal of Computer and System Sciences" }
@string{ compsyssci = "J. Comput. System Sci." }
@string{ computing = "Computing" }
@string{ contempmath = "Contemporary Mathematics" }
@string{ contempmath = "Contemp. Math." }
@string{ crelle = "Crelle's Journal" }
@string{ giornalemath = "Giornale di Mathematiche" }
@string{ giornalemath = "Giorn. Mat." }
@string{ computer = "{IEEE} Computer" }
@string{ ieeetranscomp = "{IEEE} Transactions on Computers" }
@string{ ieeetranscomp = "{IEEE} Trans. Comput." }
@string{ ieeetransac = "{IEEE} Transactions on Automatic Control" }
@string{ ieeetransac = "{IEEE} Trans. Automat. Control" }
@string{ ieeespec = "{IEEE} Spectrum" }
@string{ procieee = "Proceedings of the {IEEE}" }
@string{ procieee = "Proc. {IEEE}" }
@string{ ieeetransaeroelec = "{IEEE} Transactions on Aerospace and Electronic Systems" }
@string{ ieeetransaeroelec = "{IEEE} Trans. Aerospace Electron. Systems" }
@string{ imanumerana = "{IMA} Journal of Numerical Analysis" }
@string{ imanumerana = "{IMA} J. Numer. Anal." }
@string{ infproclet = "Information Processing Letters" }
@string{ infproclet = "Inform. Process. Lett." }
@string{ instmathapp = "Journal of the Institute of Mathematics and its Applications" }
@string{ instmathapp = "J. Inst. Math. Appl." }
@string{ intcontrol = "International Journal of Control" }
@string{ intcontrol = "Internat. J. Control" }
@string{ intnumereng = "International Journal for Numerical Methods in Engineering" }
@string{ intnumereng = "Internat. J. Numer. Methods Engrg." }
@string{ intsuper = "International Journal of Supercomputing Applications" }
@string{ intsuper = "Internat. J. Supercomputing Applic." }
@string{ kibernetika = "Kibernetika" }
@string{ jresnatburstand = "Journal of Research of the National Bureau of Standards" }
@string{ jresnatburstand = "J. Res. Nat. Bur. Standards" }
@string{ linalgapp = "Linear Algebra and its Applications" }
@string{ linalgapp = "Linear Algebra Appl." }
@string{ mathanaappl = "Journal of Mathematical Analysis and Applications" }
@string{ mathanaappl = "J. Math. Anal. Appl." }
@string{ mathannalen = "Mathematische Annalen" }
@string{ mathannalen = "Math. Ann." }
@string{ mathphys = "Journal of Mathematical Physics" }
@string{ mathphys = "J. Math. Phys." }
@string{ mathcomp = "Mathematics of Computation" }
@string{ mathcomp = "Math. Comp." }
@string{ mathscand = "Mathematica Scandinavica" }
@string{ mathscand = "Math. Scand." }
@string{ tablesaidscomp = "Mathematical Tables and Other Aids to Computation" }
@string{ tablesaidscomp = "Math. Tables Aids Comput." }
@string{ numermath = "Numerische Mathematik" }
@string{ numermath = "Numer. Math." }
@string{ pacificmath = "Pacific Journal of Mathematics" }
@string{ pacificmath = "Pacific J. Math." }
@string{ pardistcomp = "Journal of Parallel and Distributed Computing" }
@string{ pardistcomp = "J. Parallel and Distrib. Comput." }
@string{ parcomputing = "Parallel Computing" }
@string{ parcomputing = "Parallel Comput." }
@string{ philmag = "Philosophical Magazine" }
@string{ philmag = "Philos. Mag." }
@string{ procnas = "Proceedings of the National Academy of Sciences of the USA" }
@string{ procnas = "Proc. Nat. Acad. Sci. U. S. A." }
@string{ psychometrika = "Psychometrika" }
@string{ quartmath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
@string{ quartmath = "Quart. J. Math. Oxford Ser. (2)" }
@string{ quartapplmath = "Quarterly of Applied Mathematics" }
@string{ quartapplmath = "Quart. Appl. Math." }
@string{ revueinststat = "Review of the International Statisical Institute" }
@string{ revueinststat = "Rev. Inst. Internat. Statist." }
@string{ jsiam = "Journal of the Society for Industrial and Applied Mathematics" }
@string{ jsiam = "J. Soc. Indust. Appl. Math." }
@string{ jsiamb = "Journal of the Society for Industrial and Applied Mathematics, Series B, Numerical Analysis" }
@string{ jsiamb = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
@string{ siamalgmeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
@string{ siamalgmeth = "{SIAM} J. Algebraic Discrete Methods" }
@string{ siamappmath = "{SIAM} Journal on Applied Mathematics" }
@string{ siamappmath = "{SIAM} J. Appl. Math." }
@string{ siamcomp = "{SIAM} Journal on Computing" }
@string{ siamcomp = "{SIAM} J. Comput." }
@string{ siammatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
@string{ siammatrix = "{SIAM} J. Matrix Anal. Appl." }
@string{ siamnumanal = "{SIAM} Journal on Numerical Analysis" }
@string{ siamnumanal = "{SIAM} J. Numer. Anal." }
@string{ siamreview = "{SIAM} Review" }
@string{ siamreview = "{SIAM} Rev." }
@string{ siamscistat = "{SIAM} Journal on Scientific and Statistical Computing" }
@string{ siamscistat = "{SIAM} J. Sci. Statist. Comput." }
@string{ softpracexp = "Software Practice and Experience" }
@string{ softpracexp = "Software Prac. Experience" }
@string{ statscience = "Statistical Science" }
@string{ statscience = "Statist. Sci." }
@string{ techno = "Technometrics" }
@string{ ussrcompmathphys = "{USSR} Computational Mathematics and Mathematical Physics" }
@string{ ussrcompmathphys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
@string{ vlsicompsys = "Journal of {VLSI} and Computer Systems" }
@string{ vlsicompsys = "J. {VLSI} Comput. Syst." }
@string{ zangewmathmech = "Zeitschrift fur Angewandte Mathematik und Mechanik" }
@string{ zangewmathmech = "Z. Angew. Math. Mech." }
@string{ zangewmathphys = "Zeitschrift fur Angewandte Mathematik und Physik" }
@string{ zangewmathphys = "Z. Angew. Math. Phys." }
@string{ academic = "Academic Press" }
@string{ acmpress = "{ACM} Press" }
@string{ adamhilger = "Adam Hilger" }
@string{ addisonwesley = "Addison-Wesley" }
@string{ allynbacon = "Allyn and Bacon" }
@string{ ams = "American Mathematical Society" }
@string{ birkhauser = "Birkha{\"u}ser" }
@string{ cambridgepress = "Cambridge University Press" }
@string{ chelsea = "Chelsea" }
@string{ claredonpress = "Claredon Press" }
@string{ doverpub = "Dover Publications" }
@string{ eyolles = "Eyolles" }
@string{ holtrinehartwinston = "Holt, Rinehart and Winston" }
@string{ interscience = "Interscience" }
@string{ johnshopkinspress = "The Johns Hopkins University Press" }
@string{ johnwileysons = "John Wiley and Sons" }
@string{ macmillan = "Macmillan" }
@string{ mathworks = "The Math Works Inc." }
@string{ mcgrawhill = "McGraw-Hill" }
@string{ natburstd = "National Bureau of Standards" }
@string{ northholland = "North-Holland" }
@string{ oxfordpress = "Oxford University Press" }
@string{ pergamonpress = "Pergamon Press" }
@string{ plenumpress = "Plenum Press" }
@string{ prenticehall = "Prentice-Hall" }
@string{ siampub = "{SIAM} Publications" }
@string{ springer = "Springer-Verlag" }
@string{ texaspress = "University of Texas Press" }
@string{ vannostrand = "Van Nostrand" }
@string{ whfreeman = "W. H. Freeman and Co." }
@article{TINN2023100729,
  title = {Fine-tuning large neural language models for biomedical natural language processing},
  journal = {Patterns},
  volume = {4},
  number = {4},
  pages = {100729},
  year = {2023},
  issn = {2666-3899},
  doi = {https://doi.org/10.1016/j.patter.2023.100729},
  author = {Tinn, Robert and Cheng, Hao and Gu, Yu and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  keywords = {natural language processing, L01.224.050.375.580, biomedical language and understanding benchmark, BLURB},
  abstract = {Summary Large neural language models have transformed modern natural language processing (NLP) applications. However, fine-tuning such models for specific tasks remains challenging as model size increases, especially with small labeled datasets, which are common in biomedical NLP. We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that fine-tuning performance may be sensitive to pretraining settings and conduct an exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for low-resource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT-BASE models, while layerwise decay is more effective for BERT-LARGE and ELECTRA models. For low-resource text similarity tasks, such as BIOSSES, reinitializing the top layers is the optimal strategy. Overall, domain-specific vocabulary and pretraining facilitate robust models for fine-tuning. Based on these findings, we establish a new state of the art on a wide range of biomedical NLP applications.}
}
@article{Maharjan2024-ScientificReports,
  author = {Maharjan, Jenish and Garikipati, Anurag and Singh, Navan Preet and Cyrus, Leo and Sharma, Mayank and Ciobanu, Madalina and Barnes, Gina and Thapa, Rahul and Mao, Qingqing and Das, Ritankar},
  doi = {10.1038/s41598-024-64827-6},
  id = {Maharjan2024},
  isbn = {2045-2322},
  journal = {Scientific Reports},
  number = {1},
  pages = {14156},
  title = {OpenMedLM: prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models},
  volume = {14},
  year = {2024},
  bdsk-url-1 = {https://doi.org/10.1038/s41598-024-64827-6}
}
@article{Wang2023-Jamia,
  author = {Wang, Andy and Liu, Cong and Yang, Jingye and Weng, Chunhua},
  title = {{Fine-tuning large language models for rare disease concept normalization}},
  journal = {Journal of the American Medical Informatics Association},
  volume = {31},
  number = {9},
  pages = {2076-2083},
  year = {2024},
  month = jun,
  issn = {1527-974X},
  doi = {10.1093/jamia/ocae133},
  month_numeric = {6}
}
@article{Giuf2024,
  author = {Giuffrè, Mauro and Kresevic, Simone and Pugliese, Nicola and You, Kisung and Shung, Dennis L.},
  title = {Optimizing large language models in digestive disease: strategies and challenges to improve clinical outcomes},
  journal = {Liver International},
  volume = {44},
  number = {9},
  pages = {2114-2124},
  keywords = {ChatGPT, Fine-tuning, Large Language Models, LLMs, Retrieval Augmented Generation, Supervised Fine-Tuning, RHLF, Reinforcement Learning from Human Feedback, In-context Learning},
  doi = {https://doi.org/10.1111/liv.15974},
  year = {2024}
}
@article{Kresevic2024-NPJ,
  author = {Kresevic, Simone and Giuffr{\`e}, Mauro and Ajcevic, Milos and Accardo, Agostino and Croc{\`e}, Lory S. and Shung, Dennis L.},
  isbn = {2398-6352},
  journal = {npj Digital Medicine},
  number = {1},
  pages = {102},
  title = {Optimization of hepatological clinical guidelines interpretation by large language models: a retrieval augmented generation-based framework},
  url = {https://doi.org/10.1038/s41746-024-01091-y},
  volume = {7},
  year = {2024},
  bdsk-url-1 = {https://doi.org/10.1038/s41746-024-01091-y}
}
@article{Clusmann-CommMed2023,
  author = {Clusmann, Jan and Kolbinger, Fiona R. and Muti, Hannah Sophie and Carrero, Zunamys I. and Eckardt, Jan-Niklas and Laleh, Narmin Ghaffari and L{\"o}ffler, Chiara Maria Lavinia and Schwarzkopf, Sophie-Caroline and Unger, Michaela and Veldhuizen, Gregory P. and Wagner, Sophia J. and Kather, Jakob Nikolas},
  doi = {10.1038/s43856-023-00370-1},
  isbn = {2730-664X},
  journal = {Communications Medicine},
  number = {1},
  pages = {141},
  title = {The future landscape of large language models in medicine},
  volume = {3},
  year = {2023},
  bdsk-url-1 = {https://doi.org/10.1038/s43856-023-00370-1}
}
@inproceedings{telmed2024,
  author = {Montagna, Sara and Aguzzi, Gianluca and Ferretti, Stefano and Pengo, Martino Francesco and Klopfenstein, Lorenz Cuno and Ungolo, Michelangelo and Magnini, Matteo},
  booktitle = {2024 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)},
  title = {{LLM-based Solutions for Healthcare Chatbots: a Comparative Analysis}},
  year = {2024},
  volume = {},
  number = {},
  pages = {346-351},
  keywords = {Pervasive computing;Privacy;Filtering;Conferences;Computational modeling;Medical services;Chatbots;Large Language Model;Medical Chatbot;Chronic Disease Management},
  doi = {10.1109/PerComWorkshops59983.2024.10503257}
}
@inproceedings{llm-goodit2023,
  author = {Montagna, Sara and Ferretti, Stefano and Klopfenstein, Lorenz Cuno and Florio, Antonio and Pengo, Martino Francesco},
  title = {Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management},
  year = {2023},
  isbn = {9798400701160},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3582515.3609536},
  booktitle = {Proceedings of the 2023 ACM Conference on Information Technology for Social Good},
  pages = {205–212},
  keywords = {personal data store, chatbot, hypertension, healthcare data privacy},
  location = {Lisbon, Portugal},
  series = {GoodIT '23}
}
@article{lancet2023,
  author = {Li, Hanzhou and Moon, John T and Purkayastha, Saptarshi and Celi, Leo Anthony and Trivedi, Hari and Gichoya, Judy W},
  doi = {10.1016/S2589-7500(23)00083-3},
  isbn = {2589-7500},
  journal = {The Lancet Digital Health},
  journal1 = {The Lancet Digital Health},
  month = {2024/01/22},
  number = {6},
  pages = {e333--e335},
  publisher = {Elsevier},
  title = {Ethics of large language models in medicine and medical research},
  type = {doi: 10.1016/S2589-7500(23)00083-3},
  volume = {5},
  year = {2023},
  year1 = {2023},
  bdsk-url-1 = {https://doi.org/10.1016/S2589-7500(23)00083-3}
}
@article{EthicsLLMs-NatureDigitalMedicine2024,
  abstract = {With the introduction of ChatGPT, Large Language Models (LLMs) have received enormous attention in healthcare. Despite potential benefits, researchers have underscored various ethical implications. While individual instances have garnered attention, a systematic and comprehensive overview of practical applications currently researched and ethical issues connected to them is lacking. Against this background, this work maps the ethical landscape surrounding the current deployment of LLMs in medicine and healthcare through a systematic review. Electronic databases and preprint servers were queried using a comprehensive search strategy which generated 796 records. Studies were screened and extracted following a modified rapid review approach. Methodological quality was assessed using a hybrid approach. For 53 records, a meta-aggregative synthesis was performed. Four general fields of applications emerged showcasing a dynamic exploration phase. Advantages of using LLMs are attributed to their capacity in data analysis, information provisioning, support in decision-making or mitigating information loss and enhancing information accessibility. However, our study also identifies recurrent ethical concerns connected to fairness, bias, non-maleficence, transparency, and privacy. A distinctive concern is the tendency to produce harmful or convincing but inaccurate content. Calls for ethical guidance and human oversight are recurrent. We suggest that the ethical guidance debate should be reframed to focus on defining what constitutes acceptable human oversight across the spectrum of applications. This involves considering the diversity of settings, varying potentials for harm, and different acceptable thresholds for performance and certainty in healthcare. Additionally, critical inquiry is needed to evaluate the necessity and justification of LLMs'current experimental use.},
  author = {Haltaufderheide, Joschka and Ranisch, Robert},
  date = {2024/07/08},
  date-added = {2024-09-27 11:48:41 +0200},
  date-modified = {2024-09-27 11:48:41 +0200},
  doi = {10.1038/s41746-024-01157-x},
  id = {Haltaufderheide2024},
  isbn = {2398-6352},
  journal = {npj Digital Medicine},
  number = {1},
  pages = {183},
  title = {The ethics of ChatGPT in medicine and healthcare: a systematic review on Large Language Models (LLMs)},
  volume = {7},
  year = {2024},
  bdsk-url-1 = {https://doi.org/10.1038/s41746-024-01157-x}
}
@article{Chatbot-JAMAIntMed2023,
  author = {Ayers, John W and Poliak, Adam and Dredze, Mark and Leas, Eric C and Zhu, Zechariah and Kelley, Jessica B and Faix, Dennis J and Goodman, Aaron M and Longhurst, Christopher A and Hogarth, Michael and Smith, Davey M},
  issn = {2168-6114 (Electronic); 2168-6106 (Print); 2168-6106 (Linking)},
  jid = {101589534},
  journal = {JAMA Internal Medicine},
  title = {Comparing Physician and Artificial Intelligence Chatbot Responses to Patient Questions Posted to a Public Social Media Forum.},
  year = {2023},
  doi = {10.1001/jamainternmed.2023.1838},
  bdsk-url-1 = {https://doi.org/10.1001/jamainternmed.2023.1838}
}
@inproceedings{DBLP:conf/naacl/DevlinCLT19,
  author = {Devlin, Jacob and Chang, Ming{-}Wei and Lee, Kenton and Toutanova, Kristina},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  title = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  year = {2019},
  doi = {10.18653/V1/N19-1423},
  timestamp = {Mon, 26 Sep 2022 12:21:55 +0200},
  biburl = {https://dblp.org/rec/conf/naacl/DevlinCLT19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{DBLP:books/cu/LeskovecRU14,
  author = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey D.},
  title = {Mining of Massive Datasets, 2nd Ed},
  publisher = {Cambridge University Press},
  year = {2014},
  url = {http://www.mmds.org/},
  isbn = {978-1107077232},
  timestamp = {Wed, 10 Jul 2019 10:47:04 +0200},
  biburl = {https://dblp.org/rec/books/cu/LeskovecRU14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/ftir/RobertsonZ09,
  author = {Robertson, Stephen E. and Zaragoza, Hugo},
  title = {The Probabilistic Relevance Framework: {BM25} and Beyond},
  journal = {Found. Trends Inf. Retr.},
  volume = {3},
  number = {4},
  pages = {333--389},
  year = {2009},
  doi = {10.1561/1500000019},
  timestamp = {Thu, 14 Oct 2021 08:51:12 +0200},
  biburl = {https://dblp.org/rec/journals/ftir/RobertsonZ09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2403-14608,
  author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
  title = {Parameter-Efficient Fine-Tuning for Large Models: {A} Comprehensive Survey},
  journal = {CoRR},
  volume = {abs/2403.14608},
  year = {2024},
  doi = {10.48550/ARXIV.2403.14608},
  eprinttype = {arXiv},
  eprint = {2403.14608},
  timestamp = {Fri, 12 Apr 2024 20:43:45 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-2403-14608.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{peft,
  title = {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author = {Mangrulkar, Sourab and Gugger, Sylvain and Debut, Lysandre and Belkada, Younes and Paul, Sayak and Bossan, Benjamin},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year = {2022}
}
@inproceedings{DBLP:conf/iclr/HuSWALWWC22,
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen{-}Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year = {2022},
  url = {https://openreview.net/forum?id=nZeVKeeFYf9},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl = {https://dblp.org/rec/conf/iclr/HuSWALWWC22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/nips/DettmersPHZ23,
  author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  editor = {Oh, Alice and Naumann, Tristan and Globerson, Amir and Saenko, Kate and Hardt, Moritz and Levine, Sergey},
  title = {QLoRA: Efficient Finetuning of Quantized LLMs},
  booktitle = {Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023},
  year = {2023},
  url = {http://papers.nips.cc/paper\_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html},
  timestamp = {Fri, 01 Mar 2024 16:26:19 +0100},
  biburl = {https://dblp.org/rec/conf/nips/DettmersPHZ23.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/sigir/CormackCB09,
  author = {Cormack, Gordon V. and Clarke, Charles L. A. and B{\"{u}}ttcher, Stefan},
  editor = {Allan, James and Aslam, Javed A. and Sanderson, Mark and Zhai, ChengXiang and Zobel, Justin},
  title = {Reciprocal rank fusion outperforms condorcet and individual rank learning methods},
  booktitle = {Proceedings of the 32nd Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, {SIGIR} 2009, Boston, MA, USA, July 19-23, 2009},
  pages = {758--759},
  publisher = {{ACM}},
  year = {2009},
  doi = {10.1145/1571941.1572114},
  timestamp = {Wed, 14 Nov 2018 10:58:10 +0100},
  biburl = {https://dblp.org/rec/conf/sigir/CormackCB09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@software{Chase_LangChain_2022,
  author = {Chase, Harrison},
  month = oct,
  title = {{LangChain}},
  url = {https://github.com/langchain-ai/langchain},
  year = {2022},
  month_numeric = {10}
}
@inproceedings{DBLP:conf/sigir/CarbonellG98,
  author = {Carbonell, Jaime G. and Goldstein, Jade},
  editor = {Croft, W. Bruce and Moffat, Alistair and van Rijsbergen, C. J. and Wilkinson, Ross and Zobel, Justin},
  title = {The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries},
  booktitle = {{SIGIR} '98: Proceedings of the 21st Annual International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval, August 24-28 1998, Melbourne, Australia},
  pages = {335--336},
  publisher = {{ACM}},
  year = {1998},
  doi = {10.1145/290941.291025},
  timestamp = {Wed, 14 Nov 2018 10:58:11 +0100},
  biburl = {https://dblp.org/rec/conf/sigir/CarbonellG98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2402-01613,
  author = {Nussbaum, Zach and Morris, John X. and Duderstadt, Brandon and Mulyar, Andriy},
  title = {Nomic Embed: Training a Reproducible Long Context Text Embedder},
  journal = {CoRR},
  volume = {abs/2402.01613},
  year = {2024},
  doi = {10.48550/ARXIV.2402.01613},
  eprinttype = {arXiv},
  eprint = {2402.01613},
  timestamp = {Fri, 09 Feb 2024 12:18:48 +0100},
  biburl = {https://dblp.org/rec/journals/corr/abs-2402-01613.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/abs-2407-21783,
  author = {et al., Abhimanyu Dubey},
  title = {{The Llama 3 Herd of Models}},
  journal = {CoRR},
  volume = {abs/2407.21783},
  year = {2024},
  doi = {10.48550/ARXIV.2407.21783},
  eprinttype = {arXiv},
  eprint = {2407.21783},
  timestamp = {Mon, 26 Aug 2024 08:08:35 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-2407-21783.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{llama3,
  doi = {10.48550/ARXIV.2407.21783},
  author = {Dubey, Abhimanyu and et. al.},
  keywords = {Artificial Intelligence (cs.AI),  Computation and Language (cs.CL),  Computer Vision and Pattern Recognition (cs.CV),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {{The Llama 3 Herd of Models}},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@misc{qwen2,
  doi = {10.48550/ARXIV.2407.10671},
  author = {Yang, An and et. al.},
  keywords = {Computation and Language (cs.CL),  Artificial Intelligence (cs.AI),  FOS: Computer and information sciences,  FOS: Computer and information sciences},
  title = {Qwen2 Technical Report},
  publisher = {arXiv},
  year = {2024},
  copyright = {arXiv.org perpetual,  non-exclusive license}
}
@misc{abdin2024phi3technicalreporthighly,
  title = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author = {Abdin, Marah and et. al.},
  year = {2024},
  eprint = {2404.14219},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2404.14219}
}
@misc{gemmateam2024gemma2improvingopen,
  title = {Gemma 2: Improving Open Language Models at a Practical Size},
  author = {Team, Gemma},
  year = {2024},
  eprint = {2408.00118},
  archiveprefix = {arXiv},
  primaryclass = {cs.CL},
  url = {https://arxiv.org/abs/2408.00118}
}
@inproceedings{Lewis-NIPS20,
  author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
  title = {Retrieval-augmented generation for knowledge-intensive NLP tasks},
  year = {2020},
  isbn = {9781713829546},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
  articleno = {793},
  numpages = {16},
  url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
  location = {Vancouver, BC, Canada},
  series = {NIPS '20}
}
